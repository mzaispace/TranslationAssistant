import gradio as gr
from datetime import datetime
import threading
import time
from typing import Optional, Generator, List, Tuple, Dict
import json

# å¯¼å…¥ä½ çš„æ¨¡å‹æ¨¡å—
try:
    from modules.pipelines.files_path import FilesPathPipelines
    from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
    import torch
    import gc
    HAS_TORCH = True
except ImportError:
    print("è­¦å‘Š: torch/transformers æ¨¡å—å¯¼å…¥å¤±è´¥")
    HAS_TORCH = False

# ============ æµå¼è¾“å‡ºç®¡ç†å™¨ ============
class StreamingManager:
    """ä¸“é—¨å¤„ç†æµå¼è¾“å‡ºçš„ç®¡ç†å™¨"""

    def __init__(self):
        self.active_streams = {}
        self.lock = threading.Lock()

    def start_stream(self, stream_id: str, stream_generator):
        """å¼€å§‹ä¸€ä¸ªæ–°çš„æµå¼ç”Ÿæˆ"""
        with self.lock:
            self.active_streams[stream_id] = {
                'generator': stream_generator,
                'start_time': time.time(),
                'last_update': time.time()
            }

    def get_stream_chunks(self, stream_id: str) -> Generator[str, None, None]:
        """ä»æµå¼ç”Ÿæˆå™¨è·å–æ–‡æœ¬å—"""
        with self.lock:
            if stream_id not in self.active_streams:
                return

            stream_data = self.active_streams[stream_id]
            generator = stream_data['generator']

        try:
            # ä»ç”Ÿæˆå™¨è·å–æ‰€æœ‰æ–‡æœ¬å—
            for chunk in generator:
                if chunk:
                    yield chunk
                # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
                with self.lock:
                    if stream_id in self.active_streams:
                        self.active_streams[stream_id]['last_update'] = time.time()

        finally:
            # æ¸…ç†æµ
            with self.lock:
                if stream_id in self.active_streams:
                    del self.active_streams[stream_id]

# ============ æ¨¡å‹ç®¡ç†å™¨ ============
class ModelManager:
    """ç®¡ç†æ¨¡å‹åŠ è½½å’Œæ¨ç†"""

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = None
        self.is_loaded = False
        self.current_model_name = None

    def load_model(self, model_name: str = "Qwen2.5-7B-Instruct", gpu_index: int = 0):
        """åŠ è½½æ¨¡å‹"""
        try:
            if not HAS_TORCH:
                return False, "æœªå®‰è£… torch/transformers"

            # æ£€æŸ¥CUDAå¯ç”¨æ€§
            if torch.cuda.is_available():
                self.device = f"cuda:{gpu_index}"
                torch_dtype = torch.float16
                print(f"ä½¿ç”¨GPU: {self.device}")
            else:
                self.device = "cpu"
                torch_dtype = torch.float32
                print("ä½¿ç”¨CPU")

            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")

            # è·å–æ¨¡å‹è·¯å¾„
            file_client = FilesPathPipelines()
            model_path = file_client.get_base_model_path(base_model_name=model_name)

            print(f"æ¨¡å‹è·¯å¾„: {model_path}")

            # åŠ è½½tokenizer
            print("åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            # åŠ è½½æ¨¡å‹
            print("åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch_dtype,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            self.is_loaded = True
            self.current_model_name = model_name

            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return True, f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}"

        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(error_msg)
            return False, error_msg

    def prepare_messages(self, user_input: str, history: List[Dict], sys_prompt: str = None) -> str:
        """å‡†å¤‡æ¶ˆæ¯æ–‡æœ¬"""
        messages = []

        if sys_prompt:
            messages.append({"role": "system", "content": sys_prompt})

        # æ·»åŠ å†å²æ¶ˆæ¯
        if history:
            # ç¡®ä¿å†å²æ ¼å¼æ­£ç¡®
            for msg in history:
                if isinstance(msg, dict) and "role" in msg and "content" in msg:
                    messages.append({"role": msg["role"], "content": msg["content"]})
                elif isinstance(msg, tuple) and len(msg) == 2:
                    # å‡è®¾æ˜¯(user, assistant)æ ¼å¼
                    user_msg, assistant_msg = msg
                    if user_msg:
                        messages.append({"role": "user", "content": user_msg})
                    if assistant_msg:
                        messages.append({"role": "assistant", "content": assistant_msg})

        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        messages.append({"role": "user", "content": user_input})

        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        return text

    def generate_stream(self, text: str, stream_id: str, max_new_tokens: int = 512):
        """ç”Ÿæˆæµå¼å“åº”"""
        if not self.is_loaded:
            yield "é”™è¯¯: æ¨¡å‹æœªåŠ è½½"
            return

        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(text, return_tensors="pt")

            # ç§»åŠ¨åˆ°è®¾å¤‡
            if "cuda" in self.device:
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
            streamer = TextIteratorStreamer(
                self.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True,
                timeout=300.0  # å¢åŠ è¶…æ—¶æ—¶é—´
            )

            # ç”Ÿæˆå‚æ•°
            gen_kwargs = {
                **inputs,
                "streamer": streamer,
                "max_new_tokens": max_new_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "do_sample": True,
                "eos_token_id": self.tokenizer.eos_token_id,
                "pad_token_id": self.tokenizer.pad_token_id,
            }

            # åœ¨åå°çº¿ç¨‹ä¸­ç”Ÿæˆ
            thread = threading.Thread(
                target=self.model.generate,
                kwargs=gen_kwargs,
                daemon=True
            )

            print(f"å¼€å§‹æµå¼ç”Ÿæˆ (ID: {stream_id})")
            thread.start()

            # ä»æµå¼ç”Ÿæˆå™¨è¯»å–
            buffer = ""
            chunk_count = 0

            for chunk in streamer:
                if chunk:
                    chunk_count += 1
                    buffer += chunk
                    print(f"æ”¶åˆ°chunk {chunk_count}: {repr(chunk[:50])}...")
                    yield chunk

            print(f"æµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")

        except Exception as e:
            error_msg = f"ç”Ÿæˆæ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ {error_msg}")
            yield error_msg

    def generate_non_stream(self, text: str, max_new_tokens: int = 512):
        """ç”Ÿæˆéæµå¼å“åº”"""
        if not self.is_loaded:
            return "é”™è¯¯: æ¨¡å‹æœªåŠ è½½"

        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(text, return_tensors="pt")

            # ç§»åŠ¨åˆ°è®¾å¤‡
            if "cuda" in self.device:
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # ç”Ÿæˆå‚æ•°
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id
                )

            # è§£ç å“åº”
            input_length = inputs['input_ids'].shape[1]
            response = self.tokenizer.decode(
                outputs[0][input_length:],
                skip_special_tokens=True
            )

            return response

        except Exception as e:
            error_msg = f"ç”Ÿæˆæ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg

# ============ èŠå¤©ç®¡ç†å™¨ ============
class ChatManager:
    def __init__(self):
        self.history = []
        self.lock = threading.Lock()
        self.stream_manager = StreamingManager()
        self.model_manager = ModelManager()

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        with self.lock:
            self.history.append({
                "role": role,
                "content": content,
                "time": datetime.now().strftime("%H:%M:%S")
            })

    def get_gradio_format(self) -> List[Tuple[str, str]]:
        """è·å–Gradioæ ¼å¼çš„èŠå¤©è®°å½•"""
        with self.lock:
            gradio_history = []

            # å¤„ç†æ¶ˆæ¯å¯¹
            user_buffer = None
            for msg in self.history:
                if msg["role"] == "user":
                    user_buffer = msg["content"]
                elif msg["role"] == "assistant" and user_buffer is not None:
                    gradio_history.append((user_buffer, msg["content"]))
                    user_buffer = None

            # å¦‚æœæœ‰æœªå›å¤çš„ç”¨æˆ·æ¶ˆæ¯
            if user_buffer is not None:
                gradio_history.append((user_buffer, None))

            return gradio_history

    def get_model_history(self, max_pairs: int = 10) -> List[Dict]:
        """è·å–æ¨¡å‹éœ€è¦çš„å†å²æ ¼å¼"""
        with self.lock:
            # è·å–æœ€è¿‘çš„æ¶ˆæ¯
            recent_messages = self.history[-max_pairs*2:] if len(self.history) > max_pairs*2 else self.history.copy()

            # è½¬æ¢ä¸ºæ¨¡å‹æ ¼å¼
            model_history = []
            for msg in recent_messages:
                if msg["role"] in ["user", "assistant"]:
                    model_history.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })

            return model_history

    def clear(self):
        """æ¸…ç©ºèŠå¤©å†å²"""
        with self.lock:
            self.history = []

# ============ å…¨å±€å®ä¾‹ ============
chat_mgr = ChatManager()

# ç³»ç»Ÿæç¤ºè¯
SYS_PROMPTS = {
    "äº§å“ç»ç†": "ä½ æ˜¯ä¸€ä½èµ„æ·±äº§å“ç»ç†ã€‚è¯·ä»äº§å“è§’åº¦æ€è€ƒé—®é¢˜ï¼Œå…³æ³¨ç”¨æˆ·ä½“éªŒã€å¸‚åœºéœ€æ±‚å’Œäº§å“è§„åˆ’ã€‚",
    "ç ”å‘": "ä½ æ˜¯ä¸€ä½èµ„æ·±ç ”å‘å·¥ç¨‹å¸ˆã€‚è¯·ä»æŠ€æœ¯è§’åº¦åˆ†æé—®é¢˜ï¼Œå…³æ³¨ç³»ç»Ÿæ¶æ„ã€æŠ€æœ¯å®ç°å’Œä»£ç è´¨é‡ã€‚",
    "é›·å†›": "ä½ æ˜¯å°ç±³åˆ›å§‹äººé›·å†›ã€‚è¯·ç”¨ä½ çš„å•†ä¸šå“²å­¦å’Œåˆ›ä¸šç»éªŒæ¥å›ç­”é—®é¢˜ï¼Œå¯ä»¥ç©¿æ’ä¸ªäººç»å†å’Œæ¯”å–»ã€‚",
    "é€šç”¨åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
}

# ============ Gradio å¤„ç†å‡½æ•° ============
def init_model_ui(model_name: str, gpu_index: int):
    """åˆå§‹åŒ–æ¨¡å‹"""
    if not HAS_TORCH:
        return "âŒ æœªå®‰è£… torch/transformersï¼Œè¯·å…ˆå®‰è£…ä¾èµ–"

    success, message = chat_mgr.model_manager.load_model(model_name, gpu_index)
    return message

def handle_stream_response(user_input: str, role: str, chat_state: list):
    """å¤„ç†æµå¼å“åº”"""
    if not user_input.strip():
        return chat_state, ""

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if not chat_mgr.model_manager.is_loaded:
        error_msg = "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåˆå§‹åŒ–æ¨¡å‹"
        chat_mgr.add_message("user", user_input)
        chat_mgr.add_message("assistant", error_msg)
        return chat_mgr.get_gradio_format(), ""

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    chat_mgr.add_message("user", user_input)

    # è·å–å†å²
    history_for_model = chat_mgr.get_model_history()

    # è·å–ç³»ç»Ÿæç¤º
    sys_prompt = SYS_PROMPTS.get(role, SYS_PROMPTS["é€šç”¨åŠ©æ‰‹"])

    # å‡†å¤‡æ¶ˆæ¯
    try:
        text = chat_mgr.model_manager.prepare_messages(
            user_input=user_input,
            history=history_for_model,
            sys_prompt=sys_prompt
        )
    except Exception as e:
        error_msg = f"å‡†å¤‡æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        chat_mgr.add_message("assistant", error_msg)
        return chat_mgr.get_gradio_format(), ""

    # ç”ŸæˆæµID
    stream_id = f"stream_{int(time.time())}_{hash(user_input)}"

    # å¼€å§‹æµå¼ç”Ÿæˆ
    stream_generator = chat_mgr.model_manager.generate_stream(text, stream_id)

    # é€æ­¥æ˜¾ç¤ºå“åº”
    full_response = ""

    # å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼ˆåŠ©æ‰‹æ¶ˆæ¯ä¸ºç©ºï¼‰
    current_display = chat_mgr.get_gradio_format()
    if current_display and current_display[-1][1] is None:
        # å·²ç»æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œæ›´æ–°å®ƒ
        user_msg = current_display[-1][0]
        current_display[-1] = (user_msg, "")
    else:
        # æ·»åŠ æ–°çš„å¯¹è¯å¯¹
        current_display.append((user_input, ""))

    yield current_display, ""

    # é€æ­¥è·å–æµå¼å“åº”
    chunk_count = 0
    try:
        for chunk in stream_generator:
            if chunk:
                chunk_count += 1
                full_response += chunk

                # æ›´æ–°æ˜¾ç¤º
                if current_display and current_display[-1][1] is not None:
                    current_display[-1] = (current_display[-1][0], full_response)
                else:
                    current_display.append((user_input, full_response))

                yield current_display, ""

        print(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")

        # å®Œæˆåæ·»åŠ åˆ°å†å²
        chat_mgr.add_message("assistant", full_response)

    except Exception as e:
        error_msg = f"æµå¼ç”Ÿæˆæ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        chat_mgr.add_message("assistant", error_msg)
        yield chat_mgr.get_gradio_format(), ""

def handle_non_stream_response(user_input: str, role: str, chat_state: list):
    """å¤„ç†éæµå¼å“åº”"""
    if not user_input.strip():
        return chat_state, ""

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if not chat_mgr.model_manager.is_loaded:
        error_msg = "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåˆå§‹åŒ–æ¨¡å‹"
        chat_mgr.add_message("user", user_input)
        chat_mgr.add_message("assistant", error_msg)
        return chat_mgr.get_gradio_format(), ""

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    chat_mgr.add_message("user", user_input)

    # è·å–å†å²
    history_for_model = chat_mgr.get_model_history()

    # è·å–ç³»ç»Ÿæç¤º
    sys_prompt = SYS_PROMPTS.get(role, SYS_PROMPTS["é€šç”¨åŠ©æ‰‹"])

    # å‡†å¤‡æ¶ˆæ¯
    try:
        text = chat_mgr.model_manager.prepare_messages(
            user_input=user_input,
            history=history_for_model,
            sys_prompt=sys_prompt
        )
    except Exception as e:
        error_msg = f"å‡†å¤‡æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        chat_mgr.add_message("assistant", error_msg)
        return chat_mgr.get_gradio_format(), ""

    # ç”Ÿæˆå“åº”
    response = chat_mgr.model_manager.generate_non_stream(text)

    # æ·»åŠ åˆ°å†å²
    chat_mgr.add_message("assistant", response)

    return chat_mgr.get_gradio_format(), ""

def clear_chat():
    """æ¸…ç©ºèŠå¤©"""
    chat_mgr.clear()
    return [], ""

def export_chat():
    """å¯¼å‡ºèŠå¤©è®°å½•"""
    if not chat_mgr.history:
        return "æš‚æ— èŠå¤©è®°å½•"

    export_text = "=" * 60 + "\n"
    export_text += "AIå¯¹è¯åŠ©æ‰‹ - èŠå¤©è®°å½•å¯¼å‡º\n"
    export_text += f"å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    export_text += "=" * 60 + "\n\n"

    for i, msg in enumerate(chat_mgr.history, 1):
        role_icon = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        export_text += f"{i}. [{msg['time']}] {role_icon} {msg['role'].upper()}:\n"
        export_text += f"{msg['content']}\n\n"

    return export_text

def update_model_status():
    """æ›´æ–°æ¨¡å‹çŠ¶æ€"""
    if chat_mgr.model_manager.is_loaded:
        return f"âœ… æ¨¡å‹å·²åŠ è½½: {chat_mgr.model_manager.current_model_name}"
    else:
        return "âŒ æ¨¡å‹æœªåŠ è½½"

# ============ Gradio UI ============
def create_webui():
    """åˆ›å»ºWebç•Œé¢"""

    with gr.Blocks(
            title="AIå¯¹è¯åŠ©æ‰‹",
            theme=gr.themes.Soft(),
            css="""
        .chat-container {
            border: 1px solid #e0e0e0;
            border-radius: 10px;
            padding: 10px;
            background: #fafafa;
            max-height: 600px;
            overflow-y: auto;
        }
        .status-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 15px;
        }
        .debug-info {
            font-size: 12px;
            color: #666;
            font-family: monospace;
            padding: 5px;
            background: #f0f0f0;
            border-radius: 5px;
            margin-top: 5px;
        }
        """
    ) as demo:

        # æ ‡é¢˜
        gr.Markdown("# ğŸ¤– AIå¯¹è¯åŠ©æ‰‹")
        gr.Markdown("åŸºäºæœ¬åœ°å¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ")

        # çŠ¶æ€åŒºåŸŸ
        with gr.Row():
            status_box = gr.Textbox(
                label="ç³»ç»ŸçŠ¶æ€",
                value="ğŸ”„ è¯·å…ˆåˆå§‹åŒ–æ¨¡å‹",
                interactive=False,
                elem_classes="status-box",
                scale=3
            )

            refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", variant="secondary", scale=1)

        # æ¨¡å‹åˆå§‹åŒ–åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=2):
                model_dropdown = gr.Dropdown(
                    choices=["Qwen2.5-7B-Instruct", "Qwen2.5-14B-Instruct"],
                    value="Qwen2.5-7B-Instruct",
                    label="é€‰æ‹©æ¨¡å‹"
                )
            with gr.Column(scale=1):
                gpu_input = gr.Number(
                    value=0, label="GPUç´¢å¼•", minimum=0, maximum=7, step=1
                )
            with gr.Column(scale=1):
                init_btn = gr.Button("ğŸš€ åˆå§‹åŒ–æ¨¡å‹", variant="primary")

        # èŠå¤©åŒºåŸŸ
        chatbot = gr.Chatbot(
            label="å¯¹è¯è®°å½•",
            height=500,
            show_label=True,
            elem_classes="chat-container"
        )

        # è¾“å…¥åŒºåŸŸ
        with gr.Row():
            user_input = gr.Textbox(
                label="è¾“å…¥æ¶ˆæ¯",
                placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...ï¼ˆæŒ‰Ctrl+Enterå‘é€ï¼‰",
                lines=3,
                max_lines=5,
                scale=4
            )

        with gr.Row():
            submit_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary", scale=1)
            stream_toggle = gr.Checkbox(
                label="æµå¼è¾“å‡º", value=True, scale=1
            )
            role_radio = gr.Radio(
                choices=["äº§å“ç»ç†", "ç ”å‘", "é›·å†›", "é€šç”¨åŠ©æ‰‹"],
                value="äº§å“ç»ç†",
                label="è§’è‰²",
                scale=2
            )

        # æ§åˆ¶åŒºåŸŸ
        with gr.Row():
            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", scale=1)
            export_btn = gr.Button("ğŸ’¾ å¯¼å‡ºè®°å½•", variant="secondary", scale=1)

        export_output = gr.Textbox(
            label="å¯¼å‡ºå†…å®¹",
            lines=8,
            visible=False
        )

        # è°ƒè¯•ä¿¡æ¯
        debug_info = gr.Textbox(
            label="è°ƒè¯•ä¿¡æ¯",
            value="",
            interactive=False,
            visible=False,
            elem_classes="debug-info"
        )

        # äº‹ä»¶å¤„ç†å‡½æ•°
        def handle_submit(user_text, role, stream, chat_state):
            """å¤„ç†æ¶ˆæ¯æäº¤"""
            if not user_text.strip():
                return chat_state, ""

            if stream:
                # æµå¼è¾“å‡º - ä½¿ç”¨ç”Ÿæˆå™¨
                for new_chatbot, _ in handle_stream_response(user_text, role, chat_state):
                    return new_chatbot, ""
                return chat_state, ""
            else:
                # éæµå¼è¾“å‡º
                new_chatbot, _ = handle_non_stream_response(user_text, role, chat_state)
                return new_chatbot, ""

        def handle_enter(user_text, role, stream, chat_state):
            """å¤„ç†å›è½¦é”®"""
            return handle_submit(user_text, role, stream, chat_state)

        # ç»‘å®šäº‹ä»¶
        init_btn.click(
            fn=init_model_ui,
            inputs=[model_dropdown, gpu_input],
            outputs=status_box
        ).then(
            fn=update_model_status,
            inputs=[],
            outputs=status_box
        )

        refresh_btn.click(
            fn=update_model_status,
            inputs=[],
            outputs=status_box
        )

        submit_btn.click(
            fn=handle_submit,
            inputs=[user_input, role_radio, stream_toggle, chatbot],
            outputs=[chatbot, user_input]
        )

        user_input.submit(
            fn=handle_enter,
            inputs=[user_input, role_radio, stream_toggle, chatbot],
            outputs=[chatbot, user_input]
        )

        clear_btn.click(
            fn=clear_chat,
            inputs=[],
            outputs=[chatbot, user_input]
        )

        export_btn.click(
            fn=export_chat,
            inputs=[],
            outputs=export_output
        ).then(
            fn=lambda: gr.update(visible=True),
            inputs=[],
            outputs=[export_output]
        )

        # é¡µé¢åŠ è½½æ—¶æ˜¾ç¤ºçŠ¶æ€
        demo.load(
            fn=update_model_status,
            inputs=[],
            outputs=status_box
        )

    return demo

# ============ ä¸»ç¨‹åº ============
def main():
    """ä¸»ç¨‹åº"""
    import argparse

    parser = argparse.ArgumentParser(description="AIå¯¹è¯åŠ©æ‰‹")
    parser.add_argument("--model", type=str, default="Qwen2.5-7B-Instruct", help="æ¨¡å‹åç§°")
    parser.add_argument("--gpu", type=int, default=0, help="GPUç´¢å¼•")
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ¤– AIå¯¹è¯åŠ©æ‰‹å¯åŠ¨ä¸­...")
    print(f"ğŸ“¦ æ¨¡å‹: {args.model}")
    print(f"âš¡ GPU: {args.gpu}" if HAS_TORCH and torch.cuda.is_available() else "âš¡ è®¾å¤‡: CPU")
    print(f"ğŸŒ ç«¯å£: {args.port}")
    print("=" * 60)

    if args.debug:
        print("ğŸ”§ è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    # æ£€æŸ¥ä¾èµ–
    if not HAS_TORCH:
        print("âŒ ç¼ºå°‘ä¾èµ–: è¯·å®‰è£… torch å’Œ transformers")
        print("    pip install torch transformers")
        return

    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    demo = create_webui()

    print(f"\nâœ… å¯åŠ¨æˆåŠŸï¼è¯·è®¿é—®: http://localhost:{args.port}")
    print("   å¦‚æœé¡µé¢æ— æ³•åŠ è½½ï¼Œè¯·æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨")
    print("   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")

    try:
        demo.launch(
            server_name="0.0.0.0",
            server_port=args.port,
            share=args.share,
            debug=args.debug,
            show_error=True,
            quiet=not args.debug
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")

if __name__ == "__main__":
    main()

