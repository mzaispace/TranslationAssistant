import os
import chainlit as cl
import asyncio
from chainlit.input_widget import Select
from dotenv import load_dotenv


load_dotenv()

ui_exe_file_path = __file__


CONFIG = {
    "local_model_name": os.getenv("LOCAL_MODEL_NAME", "Qwen2.5-7B-Instruct"),
    "gpu_index": 0,
    "max_history": 10,
    "use_mock_model": False,
    # --- åœ¨çº¿æ¨¡å‹é…ç½®ä»ç¯å¢ƒå˜é‡è¯»å– ---
    "openai_api_key": os.getenv("OPENAI_API_KEY"),
    "openai_base_url": os.getenv("OPENAI_BASE_URL"),
    "openai_model": os.getenv("OPENAI_MODEL", "gpt-4o"),
}



ROLE_NAME_TO_KEY = {"äº§å“è§†è§’ -> è¯‘ç»™å¼€å‘": "to_dev", "å¼€å‘è§†è§’ -> è¯‘ç»™äº§å“": "to_prod"}
MODEL_OPTIONS = {"æœ¬åœ°å¼•æ“": "local", "åœ¨çº¿å¼•æ“ (OpenAI)": "openai"}

dev_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†äº§å“ç»ç†çš„ã€ä¸šåŠ¡æè¿°ã€‘ç¿»è¯‘æˆã€æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‘ã€‚
è¾“å‡ºå¿…é¡»åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š
1. **æŠ€æœ¯å»ºæ¨¡**ï¼šæ¨èç®—æ³•å»ºè®®ã€æ•°æ®è¡¨ç»“æ„ç®€è¿°ã€‚
2. **æ•°æ®é“¾è·¯**ï¼šæ•°æ®æ¥æºã€å¤„ç†é€»è¾‘ã€‚
3. **éåŠŸèƒ½éœ€æ±‚**ï¼šQPSè¦æ±‚ã€å»¶è¿Ÿæ§åˆ¶ã€ç¼“å­˜ç­–ç•¥ã€‚
4. **å¼€å‘é¢„ä¼°**ï¼šæ ¸å¿ƒæ¨¡å—ã€éš¾ç‚¹åŠå·¥ä½œé‡é¢„ä¼°ã€‚"""

prod_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±äº§å“ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç ”å‘çš„ã€æŠ€æœ¯å®ç°/ä¼˜åŒ–ã€‘ç¿»è¯‘æˆã€äº§å“ä¸šåŠ¡ä»·å€¼ã€‘ã€‚
è¾“å‡ºå¿…é¡»åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š
1. **ç”¨æˆ·ä½“éªŒ**ï¼šå“åº”å˜å¿«äº†å¤šå°‘ï¼Ÿæ“ä½œè·¯å¾„æ˜¯å¦ç¼©çŸ­ï¼Ÿ
2. **å•†ä¸šä»·å€¼**ï¼šæ”¯æŒå¤šå¤§çš„ä¸šåŠ¡å¢é•¿ï¼Ÿæˆæœ¬é™ä½å¤šå°‘ï¼Ÿ
3. **å¸‚åœºç«äº‰åŠ›**ï¼šæ­¤é¡¹æ”¹è¿›å¦‚ä½•é¢†å…ˆäºç«å“ï¼Ÿ
4. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼šåŸºäºæ­¤æŠ€æœ¯æå‡ï¼Œäº§å“å±‚é¢å¯ä»¥åšå“ªäº›æ–°çš„å°è¯•ï¼Ÿ"""

ROLE_MAP = {
    "to_dev": {"name": "ç ”å‘æŠ€æœ¯è§†è§’", "icon": "âš™ï¸", "description": "ä¸šåŠ¡->æŠ€æœ¯", "prompt": dev_prompt},
    "to_prod": {"name": "äº§å“ä¸šåŠ¡è§†è§’", "icon": "ğŸ“ˆ", "description": "æŠ€æœ¯->ä¸šåŠ¡", "prompt": prod_prompt}
}

# ============ 2. æ¨¡å‹å¼•æ“ç±»å®šä¹‰ ============

# æ¨¡æ‹Ÿæ¨¡å‹
class MockModel:
    def generate_response(self, user_query, history, sys_prompt, stream=True):
        yield f"ã€æ¨¡æ‹Ÿå›å¤ã€‘\nå½“å‰èº«ä»½ï¼š{sys_prompt[:20]}...\nè¾“å…¥ï¼š{user_query}"


from modules.llm.online_model import OpenAIModel


# ============ 3. åˆå§‹åŒ–ä¸å…¨å±€å˜é‡ ============
local_engine = None
openai_engine = None

async def init_engines():
    global local_engine, openai_engine

    # 1. åˆå§‹åŒ–åœ¨çº¿å¼•æ“
    try:
        openai_engine = OpenAIModel(
            api_key=CONFIG["openai_api_key"],
            base_url=CONFIG["openai_base_url"],
            model=CONFIG["openai_model"]
        )
        msg_online = "âœ… åœ¨çº¿ OpenAI å¼•æ“å°±ç»ª"
    except Exception as e:
        msg_online = f"âŒ åœ¨çº¿å¼•æ“å¯åŠ¨å¤±è´¥: {str(e)}"

    # 2. åˆå§‹åŒ–æœ¬åœ°å¼•æ“
    if CONFIG["use_mock_model"]:
        local_engine = MockModel()
        msg_local = "âœ… æ¨¡æ‹Ÿå¼•æ“åŠ è½½"
    else:
        try:
            from modules.llm.local_model import LocalModelChat
            local_engine = LocalModelChat(base_model_name=CONFIG["local_model_name"], gpu_index=CONFIG["gpu_index"])
            msg_local = "âœ… æœ¬åœ°å¼•æ“å°±ç»ª"
        except Exception as e:
            local_engine = MockModel()
            msg_local = f"âš ï¸ æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œé™çº§ä¸ºæ¨¡æ‹Ÿ: {str(e)}"

    return f"{msg_local} | {msg_online}"

# ============ 4. Chainlit é€»è¾‘ ============

@cl.on_chat_start
async def start_chat():
    cl.user_session.set("history", [])
    cl.user_session.set("role", "to_dev")
    cl.user_session.set("engine_type", "openai") # é»˜è®¤åœ¨çº¿

    # è®¾ç½®ä¾§è¾¹æ ï¼šè§’è‰²åˆ‡æ¢ + æ¨¡å‹åˆ‡æ¢
    await cl.ChatSettings([
        Select(id="role_select", label="ğŸ”„ ç¿»è¯‘æ–¹å‘", values=list(ROLE_NAME_TO_KEY.keys()), initial_index=0),
        Select(id="engine_select", label="ğŸ¤– æ¨ç†å¼•æ“", values=list(MODEL_OPTIONS.keys()), initial_index=0)
    ]).send()

    # å‘é€æ¬¢è¿è¯­å’Œå¿«æ·æŒ‰é’®
    actions = [
        cl.Action(name="switch", payload={"v": "to_dev"}, label="ğŸ“¢ è¯‘ç»™å¼€å‘"),
        cl.Action(name="switch", payload={"v": "to_prod"}, label="ğŸ’¡ è¯‘ç»™äº§å“"),
        cl.Action(name="clear", payload={"v": "clear"}, label="ğŸ—‘ï¸ æ¸…ç©ºå†å²")
    ]
    await cl.Message(content="# ğŸš€ ç ”å‘-äº§å“ ç¿»è¯‘åŠ©æ‰‹\nè¯·åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„æè¿°ï¼Œæˆ–åœ¨ä¾§è¾¹æ åˆ‡æ¢å¼•æ“ã€‚", actions=actions).send()

    # åˆå§‹åŒ–å¼•æ“
    status_msg = cl.Message(content="ğŸ”„ æ­£åœ¨é¢„çƒ­ AI å¼•æ“...", author="ç³»ç»Ÿ")
    await status_msg.send()
    status_text = await init_engines()
    status_msg.content = status_text
    await status_msg.update()


@cl.on_settings_update
async def on_settings_update(settings):
    # å¤„ç†è§’è‰²åˆ‡æ¢
    if "role_select" in settings:
        cl.user_session.set("role", ROLE_NAME_TO_KEY[settings["role_select"]])
    # å¤„ç†å¼•æ“åˆ‡æ¢
    if "engine_select" in settings:
        cl.user_session.set("engine_type", MODEL_OPTIONS[settings["engine_select"]])

    await cl.Message(content=f"âš™ï¸ é…ç½®å·²æ›´æ–°ï¼š{settings.get('role_select', '')} | {settings.get('engine_select', '')}", author="ç³»ç»Ÿ").send()

@cl.action_callback("switch")
async def on_action_switch(action):
    cl.user_session.set("role", action.payload["v"])
    await cl.Message(content=f"âœ… å·²åˆ‡æ¢è‡³ï¼š{ROLE_MAP[action.payload['v']]['name']}", author="ç³»ç»Ÿ").send()

@cl.action_callback("clear")
async def on_action_clear(action):
    cl.user_session.set("history", [])
    await cl.Message(content="ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…ç©º", author="ç³»ç»Ÿ").send()

@cl.on_message
async def handle_message(message: cl.Message):
    # 1. è·å–å½“å‰çŠ¶æ€
    role_key = cl.user_session.get("role", "to_dev")
    engine_type = cl.user_session.get("engine_type", "local")
    role_config = ROLE_MAP[role_key]
    history = cl.user_session.get("history", [])

    # 2. åŒ¹é…å¼•æ“
    engine = local_engine if engine_type == "local" else openai_engine
    sleep_time = 0.005 if engine_type == "local" else  0.01

    if not engine:
        await cl.Message(content="âŒ è¯¥å¼•æ“æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥ API é…ç½®ã€‚", author="ç³»ç»Ÿ").send()
        return

    # 3. å‡†å¤‡ UI
    msg = cl.Message(content="", author=f"{role_config['name']} ({engine_type.upper()})")
    await msg.send()

    # 4. ç”Ÿæˆå›å¤
    prefix = f"**[{role_config['name']} è½¬è¯‘ä¸­...]**\n\n"
    await msg.stream_token(prefix)

    try:
        stream = engine.generate_response(
            user_query=message.content,
            history=history,
            sys_prompt=role_config["prompt"],
            stream=True
        )

        full_response = ""
        for token in stream:
            if token:
                await msg.stream_token(token)
                full_response += token
                await asyncio.sleep(sleep_time)

        await msg.update()

        # 5. æ›´æ–°å†å²
        history.append({"role": "user", "content": message.content})
        history.append({"role": "assistant", "content": full_response})
        if len(history) > CONFIG["max_history"] * 2:
            history = history[-(CONFIG["max_history"] * 2):]
        cl.user_session.set("history", history)

    except Exception as e:
        await cl.Message(content=f"âŒ ç¿»è¯‘å‡ºé”™: {str(e)}", author="ç³»ç»Ÿ").send()

if __name__ == "__main__":
    from chainlit.cli import run_chainlit
    run_chainlit(ui_exe_file_path)

