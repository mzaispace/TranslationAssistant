import chainlit as cl
import asyncio
from chainlit.input_widget import Select, Switch

# ============ é…ç½®åŒºåŸŸ ============
CONFIG = {
    "model_name": "Qwen2.5-7B-Instruct",
    "gpu_index": 0,
    "max_history": 10,
    "use_mock_model": False,  # ã€è°ƒè¯•ç”¨ã€‘å¦‚æœä¸º Trueï¼Œå°†ä¸åŠ è½½çœŸå®æ¨¡å‹ï¼Œä»…æµ‹è¯•UI
}


ROLE_NAME_TO_KEY = {"äº§å“è§†è§’ -> è¯‘ç»™å¼€å‘": "to_dev", "å¼€å‘è§†è§’ -> è¯‘ç»™äº§å“": "to_prod"}

dev_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†äº§å“ç»ç†çš„ã€ä¸šåŠ¡æè¿°ã€‘ç¿»è¯‘æˆã€æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‘ã€‚
è¾“å‡ºå¿…é¡»åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š
1. **æŠ€æœ¯å»ºæ¨¡**ï¼šæ¨èç®—æ³•å»ºè®®ï¼ˆå¦‚ååŒè¿‡æ»¤ã€å‘é‡æ£€ç´¢ï¼‰ã€æ•°æ®è¡¨ç»“æ„ç®€è¿°ã€‚
2. **æ•°æ®é“¾è·¯**ï¼šæ•°æ®æ¥æºï¼ˆåŸ‹ç‚¹ã€ç¦»çº¿/å®æ—¶æµå¤„ç†ï¼‰ã€å¤„ç†é€»è¾‘ã€‚
3. **éåŠŸèƒ½éœ€æ±‚**ï¼šQPSè¦æ±‚ã€å»¶è¿Ÿæ§åˆ¶ã€ç¼“å­˜ç­–ç•¥ã€‚
4. **å¼€å‘é¢„ä¼°**ï¼šæ ¸å¿ƒæ¨¡å—ã€æ½œåœ¨æŠ€æœ¯éš¾ç‚¹åŠå·¥ä½œé‡è¯„ä¼°ã€‚
è¯·ä¿æŒå£å¾„ä¸“ä¸šã€ä¸¥è°¨ã€‚"""

prod_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±äº§å“ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç ”å‘çš„ã€æŠ€æœ¯å®ç°/ä¼˜åŒ–ã€‘ç¿»è¯‘æˆã€äº§å“ä¸šåŠ¡ä»·å€¼ã€‘ã€‚
è¾“å‡ºå¿…é¡»åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š
1. **ç”¨æˆ·ä½“éªŒ**ï¼šå“åº”å˜å¿«äº†å¤šå°‘ï¼Ÿæ“ä½œè·¯å¾„æ˜¯å¦ç¼©çŸ­ï¼Ÿ
2. **å•†ä¸šä»·å€¼**ï¼šæ”¯æŒå¤šå¤§çš„ä¸šåŠ¡å¢é•¿ï¼ˆå¹¶å‘å®¹é‡ï¼‰ï¼ŸæœåŠ¡å™¨æˆæœ¬é™ä½å¤šå°‘ï¼Ÿ
3. **å¸‚åœºç«äº‰åŠ›**ï¼šæ­¤é¡¹æ”¹è¿›å¦‚ä½•é¢†å…ˆäºç«å“ï¼Ÿ
4. **ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼šåŸºäºæ­¤æŠ€æœ¯æå‡ï¼Œäº§å“å±‚é¢å¯ä»¥åšå“ªäº›æ–°çš„å°è¯•ï¼Ÿ
è¯·ä¿æŒå£å¾„æ˜“æ‡‚ã€ç»“æœå¯¼å‘ã€‚"""



ROLE_MAP = {
    "to_dev": {
        "name": "ç ”å‘æŠ€æœ¯è§†è§’",
        "icon": "âš™ï¸",
        "description": "å°†ä¸šåŠ¡éœ€æ±‚è½¬åŒ–ä¸ºæŠ€æœ¯è§„æ ¼",
        "prompt": dev_prompt
    },
    "to_prod": {
        "name": "äº§å“ä¸šåŠ¡è§†è§’",
        "icon": "ğŸ“ˆ",
        "description": "å°†æŠ€æœ¯æ–¹æ¡ˆè½¬åŒ–ä¸ºå•†ä¸šä»·å€¼",
        "prompt": prod_prompt
    }
}



# ============ æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆç”¨äºæ— GPUç¯å¢ƒæµ‹è¯•UIï¼‰ ============
class MockModel:
    def generate_response(self, user_query, history, sys_prompt, stream=True):
        yield f"ã€æ¨¡æ‹Ÿå›å¤ã€‘\næ”¶åˆ°é—®é¢˜ï¼š{user_query}\n\nå½“å‰è§’è‰²è®¾å®šï¼š\n{sys_prompt[:50]}...\n\n(è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›å¤ï¼Œè¯·åœ¨ä»£ç ä¸­è®¾ç½® use_mock_model=False ä»¥åŠ è½½çœŸå®æ¨¡å‹)"



# ============ åˆå§‹åŒ–é€»è¾‘ ============
chat_model = None

async def init_model():
    global chat_model
    if CONFIG["use_mock_model"]:
        chat_model = MockModel()
        return "âœ… æ¨¡æ‹Ÿæ¨¡å‹å·²åŠ è½½ (UIæµ‹è¯•æ¨¡å¼)"

    try:
        try:
            from modules.agents.inference.local_model_infer import LocalModelChat
            import torch
        except ImportError:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°æ–‡ä»¶ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæˆ–æŠ¥é”™
            print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ¨¡å—ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
            chat_model = MockModel()
            return "âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å—ï¼Œå·²åˆ‡æ¢è‡³æ¨¡æ‹Ÿæ¨¡å¼"

        chat_model = LocalModelChat(
            base_model_name=CONFIG["model_name"],
            gpu_index=CONFIG["gpu_index"]
        )
        device = 'GPU' if torch.cuda.is_available() else 'CPU'
        return f"âœ… æ¨¡å‹å·²åŠ è½½ ({device}): {CONFIG['model_name']}"
    except Exception as e:
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"



@cl.on_chat_start
async def start_chat():
    cl.user_session.set("history", [])
    cl.user_session.set("role", "to_dev") # é»˜è®¤ï¼šè½¬è¯‘ç»™å¼€å‘çœ‹

    # è®¾ç½®ä¾§è¾¹æ è§’è‰²åˆ‡æ¢
    await cl.ChatSettings([
        Select(
            id="role_select",
            label="ğŸ”„ é€‰æ‹©ç¿»è¯‘æ–¹å‘",
            values=list(ROLE_NAME_TO_KEY.keys()),
            initial_index=0
        )
    ]).send()

    # æ¬¢è¿è¯­ä¸å¿«æ·æ“ä½œ
    actions = [
        cl.Action(name="switch", payload={"v": "to_dev"}, label="ğŸ“¢ ç ”å‘è§†è§’", description="ä¸šåŠ¡ -> æŠ€æœ¯"),
        cl.Action(name="switch", payload={"v": "to_prod"}, label="ğŸ’¡ äº§å“è§†è§’", description="æŠ€æœ¯ -> ä¸šåŠ¡"),
    ]

    await cl.Message(
        content="""# ğŸš€ ç ”å‘-äº§å“ æ²Ÿé€šç¿»è¯‘åŠ©æ‰‹
è¯·åœ¨ä¸‹æ–¹è¾“å…¥æ‚¨çš„æè¿°ï¼Œæˆ‘ä¼šä¸ºæ‚¨ç¿»è¯‘æˆå¯¹æ–¹èƒ½å¬æ‡‚çš„ä¸“ä¸šè¯­è¨€ã€‚""",
        actions=actions
    ).send()

    # 3. æ˜¾ç¤ºåŠ è½½ä¸­
    loading_msg = cl.Message(content="ğŸ”„ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...", author="System")
    await loading_msg.send()

    # 4. åŠ è½½æ¨¡å‹
    status_text = await init_model()
    loading_msg.content = status_text
    await loading_msg.update()


@cl.on_settings_update
async def on_settings_update(settings):
    new_role_name = settings["role_select"]
    new_role_key = ROLE_NAME_TO_KEY[new_role_name]
    await switch_role(new_role_key)


@cl.action_callback("switch")
async def on_action_switch(action):
    await switch_role(action.payload["v"])


async def switch_role(role_key):
    cl.user_session.set("role", role_key)
    role_info = ROLE_MAP[role_key]
    await cl.Message(content=f"âœ… å·²åˆ‡æ¢è‡³ï¼š**{role_info['name']}** ({role_info['description']})", author="ç³»ç»Ÿ").send()



@cl.action_callback("set_role_dev")
async def on_action_dev(action):
    await switch_role("dev")


@cl.action_callback("clear_history")
async def on_action_clear(action):
    cl.user_session.set("history", [])
    await cl.Message(content="ğŸ—‘ï¸ è®°å¿†å·²æ¸…é™¤ï¼Œè®©æˆ‘ä»¬é‡æ–°å¼€å§‹ã€‚", author="System").send()


@cl.on_message
async def handle_message(message: cl.Message):
    global chat_model

    if not chat_model:
        await cl.Message(content="âŒ æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å¤„ç†æ¶ˆæ¯ã€‚", author="ç³»ç»Ÿ").send()
        return

    # 1. è·å–å½“å‰çŠ¶æ€
    role_key = cl.user_session.get("role", "to_dev")
    role_config = ROLE_MAP[role_key]
    history = cl.user_session.get("history", [])

    # 2. å‡†å¤‡ UI
    author_name = role_config["name"]
    msg = cl.Message(content="", author=author_name)
    await msg.send()

    # 3. å‡†å¤‡ Prompt
    sys_prompt = role_config["prompt"]

    try:
        # 4. ç”Ÿæˆå›å¤ (æµå¼)
        # å¢åŠ ä¸€ä¸ªç¿»è¯‘ä¸­çš„å°æç¤ºå‰ç¼€
        prefix = f"**[{role_config['name']}è½¬è¯‘ä¸­...]**\n\n"
        await msg.stream_token(prefix)

        stream = chat_model.generate_response(
            user_query=message.content,
            history=history,
            sys_prompt=sys_prompt,
            stream=True
        )

        full_response = ""
        for token in stream:
            if token:
                await msg.stream_token(token)
                full_response += token
                await asyncio.sleep(0.005)

        await msg.update()

        # 5. æ›´æ–°å†å²
        history.append({"role": "user", "content": message.content})
        history.append({"role": "assistant", "content": full_response})

        # é™åˆ¶ä¸Šä¸‹æ–‡è½®æ•°
        if len(history) > CONFIG["max_history"] * 2:
            history = history[-(CONFIG["max_history"] * 2):]

        cl.user_session.set("history", history)

    except Exception as e:
        error_info = f"âŒ ç¿»è¯‘å‡ºé”™: {str(e)}"
        await cl.Message(content=error_info, author="ç³»ç»Ÿ").send()



if __name__ == "__main__":
    from chainlit.cli import run_chainlit
    run_chainlit(__file__)