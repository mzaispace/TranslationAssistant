"""
ğŸ“ app.py - AI å¯¹è¯åŠ©æ‰‹ (Chainlit å•æ–‡ä»¶ç‰ˆæœ¬)
ç®€æ´çš„ Web ç•Œé¢ï¼Œæ”¯æŒäº§å“ç»ç†/ç ”å‘å·¥ç¨‹å¸ˆåŒè§†è§’æµå¼å¯¹è¯
"""

import chainlit as cl
import sys


# ============ é…ç½®åŒºåŸŸ ============
# åœ¨è¿™é‡Œé…ç½®æ¨¡å‹è·¯å¾„å’Œè§’è‰²è®¾å®š
CONFIG = {
    "model_name": "Qwen2.5-7B-Instruct",  # é»˜è®¤æ¨¡å‹
    "gpu_index": 0,                       # GPU ç´¢å¼•
    "max_history": 6,                     # æœ€å¤§å†å²å¯¹è¯è½®æ•° (æ¯è½®2æ¡æ¶ˆæ¯)
}

# ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰ä¸åŒè§’è‰²çš„å›ç­”é£æ ¼
ROLE_PROMPTS = {
    "äº§å“ç»ç†": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§å“ç»ç†ï¼Œè¯·ä»ä»¥ä¸‹è§’åº¦å›ç­”é—®é¢˜ï¼š
1. ç”¨æˆ·éœ€æ±‚ä¸å¸‚åœºåˆ†æ
2. äº§å“åŠŸèƒ½è§„åˆ’ä¸ä¼˜å…ˆçº§
3. ç”¨æˆ·ä½“éªŒè®¾è®¡æ€è·¯
4. æ•°æ®æŒ‡æ ‡ä¸æ•ˆæœè¯„ä¼°
5. äº§å“è¿­ä»£ä¸ä¼˜åŒ–å»ºè®®

è¯·ä»¥äº§å“ç»ç†çš„ä¸“ä¸šè§†è§’ï¼Œç»™å‡ºç»“æ„æ¸…æ™°ã€å¯è½åœ°çš„å›ç­”ã€‚""",

    "ç ”å‘å·¥ç¨‹å¸ˆ": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”å‘å·¥ç¨‹å¸ˆï¼Œè¯·ä»ä»¥ä¸‹è§’åº¦å›ç­”é—®é¢˜ï¼š
1. æŠ€æœ¯æ–¹æ¡ˆé€‰å‹ä¸è¯„ä¼°
2. ç³»ç»Ÿæ¶æ„è®¾è®¡æ€è·¯
3. æ ¸å¿ƒä»£ç é€»è¾‘ä¸å®ç°
4. æ€§èƒ½ä¼˜åŒ–ä¸æ‰©å±•æ€§
5. æŠ€æœ¯é£é™©ä¸è§£å†³æ–¹æ¡ˆ

è¯·ä»¥ç ”å‘å·¥ç¨‹å¸ˆçš„ä¸“ä¸šè§†è§’ï¼Œç»™å‡ºæŠ€æœ¯å‡†ç¡®ã€å®ç°å¯è¡Œçš„å›ç­”ã€‚"""

}

# ============ æ¨¡å‹å¯¼å…¥ä¸åˆå§‹åŒ– ============
print("ğŸš€ æ­£åœ¨åˆå§‹åŒ– AI å¯¹è¯åŠ©æ‰‹...")

# å¯¼å…¥å¿…è¦çš„åº“
try:
    # å¦‚æœè·¯å¾„ä¸åŒï¼Œè¯·ä¿®æ”¹è¿™é‡Œçš„å¯¼å…¥
    from modules.pipelines.files_path import FilesPathPipelines
    from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
    from modules.agents.inference.local_model_infer import LocalModelChat
    import torch
    import gc
    HAS_DEPS = True
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…: pip install torch transformers")
    HAS_DEPS = False
    sys.exit(1)




# ============ Chainlit åº”ç”¨æ ¸å¿ƒ ============
# å…¨å±€æ¨¡å‹å®ä¾‹
chat_model = None

@cl.on_chat_start
async def start_chat():
    """èŠå¤©å¼€å§‹æ—¶æ‰§è¡Œ - åˆå§‹åŒ–æ¨¡å‹å’Œä¼šè¯çŠ¶æ€"""
    global chat_model

    # æ¬¢è¿æ¶ˆæ¯
    welcome_msg = cl.Message(
        content="""
# ğŸ¤– AI å¯¹è¯åŠ©æ‰‹

**åŠŸèƒ½ç‰¹è‰²ï¼š**
- ğŸ­ äº§å“ç»ç† / ç ”å‘å·¥ç¨‹å¸ˆåŒè§†è§’
- âš¡ å®æ—¶æµå¼è¾“å‡º
- ğŸ§  è‡ªåŠ¨ä¸Šä¸‹æ–‡è®°å¿†
- ğŸ”„ éšæ—¶åˆ‡æ¢è§’è‰²

**å¿«é€Ÿå¼€å§‹ï¼š**
1. ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆï¼ˆçº¦1-2åˆ†é’Ÿï¼‰
2. é€‰æ‹©å¯¹è¯è§’è‰²
3. å¼€å§‹èŠå¤©ï¼
""",
        author="ç³»ç»Ÿ"
    )
    await welcome_msg.send()

    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    loading_msg = cl.Message(content="ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...", author="ç³»ç»Ÿ")
    await loading_msg.send()

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        chat_model = LocalModelChat(
            base_model_name=CONFIG["model_name"],
            gpu_index=CONFIG["gpu_index"]
        )

        # æ›´æ–°åŠ è½½æ¶ˆæ¯
        loading_msg.content = "âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹å¯¹è¯ã€‚"
        await loading_msg.update()

        # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
        cl.user_session.set("history", [])
        cl.user_session.set("current_role", "äº§å“ç»ç†")  # é»˜è®¤è§’è‰²

        # æ˜¾ç¤ºè§’è‰²é€‰æ‹©æŒ‰é’®
        await show_role_selector()

    except Exception as e:
        loading_msg.content = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        await loading_msg.update()

@cl.on_message
async def handle_message(message: cl.Message):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
    global chat_model

    user_input = message.content.strip()
    if not user_input:
        return

    # è·å–å½“å‰è§’è‰²å’Œå†å²
    current_role = cl.user_session.get("current_role", "äº§å“ç»ç†")
    history = cl.user_session.get("history", [])
    sys_prompt = ROLE_PROMPTS.get(current_role, ROLE_PROMPTS["äº§å“ç»ç†"])

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    user_msg = cl.Message(content=user_input, author="æ‚¨")
    await user_msg.send()

    # åˆ›å»ºAIå›å¤æ¶ˆæ¯ï¼ˆç”¨äºæµå¼è¾“å‡ºï¼‰
    ai_msg = cl.Message(content="", author=current_role)
    await ai_msg.send()

    try:
        # è·å–æµå¼å“åº”
        stream = chat_model.generate_response(
            user_query=user_input,
            history=history,
            sys_prompt=sys_prompt,
            stream=True
        )

        # æµå¼è¾“å‡º
        full_response = ""
        async with cl.Step(name="æ€è€ƒä¸­..."):
            for chunk in stream:
                if chunk:
                    full_response += chunk
                    await ai_msg.stream_token(chunk)

        # æ›´æ–°å†å²è®°å½•
        new_history = history + [
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": full_response}
        ]

        # é™åˆ¶å†å²é•¿åº¦
        if len(new_history) > CONFIG["max_history"] * 2:
            new_history = new_history[-(CONFIG["max_history"] * 2):]

        cl.user_session.set("history", new_history)

        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        await show_status()

    except Exception as e:
        error_text = f"âŒ æŠ±æ­‰ï¼Œå‡ºé”™äº†: {str(e)}"
        await ai_msg.stream_token(error_text)
        await ai_msg.update()

async def show_role_selector():
    """æ˜¾ç¤ºè§’è‰²é€‰æ‹©å™¨"""
    role_msg = cl.Message(
        content="## ğŸ­ é€‰æ‹©å¯¹è¯è§’è‰²",
        author="ç³»ç»Ÿ"
    )

    # æ·»åŠ è§’è‰²é€‰æ‹©æŒ‰é’®
    actions = [
        cl.Action(name="select_product", value="äº§å“ç»ç†", label="ğŸ“Š äº§å“ç»ç†"),
        cl.Action(name="select_dev", value="ç ”å‘å·¥ç¨‹å¸ˆ", label="ğŸ’» ç ”å‘å·¥ç¨‹å¸ˆ")
    ]

    await role_msg.send()
    await cl.Message(content="ç‚¹å‡»æŒ‰é’®åˆ‡æ¢è§’è‰²ï¼š", actions=actions).send()

    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    await show_status()

async def show_status():
    """æ˜¾ç¤ºå½“å‰çŠ¶æ€"""
    current_role = cl.user_session.get("current_role", "äº§å“ç»ç†")
    history = cl.user_session.get("history", [])
    conversation_count = len(history) // 2

    status_text = f"""
ğŸ“Š **å½“å‰çŠ¶æ€**
- è§’è‰²ï¼š{current_role}
- å†å²ï¼š{conversation_count} è½®å¯¹è¯
- æ¨¡å‹ï¼š{CONFIG['model_name']}
"""

    # æ›´æ–°æˆ–åˆ›å»ºçŠ¶æ€æ¶ˆæ¯
    status_msg = cl.user_session.get("status_msg")
    if status_msg:
        status_msg.content = status_text
        await status_msg.update()
    else:
        status_msg = cl.Message(content=status_text, author="ç³»ç»Ÿ")
        await status_msg.send()
        cl.user_session.set("status_msg", status_msg)

# ============ è§’è‰²åˆ‡æ¢å›è°ƒ ============
@cl.action_callback("select_product")
async def on_select_product(action: cl.Action):
    """åˆ‡æ¢ä¸ºäº§å“ç»ç†"""
    await switch_role("äº§å“ç»ç†")

@cl.action_callback("select_dev")
async def on_select_dev(action: cl.Action):
    """åˆ‡æ¢ä¸ºç ”å‘å·¥ç¨‹å¸ˆ"""
    await switch_role("ç ”å‘å·¥ç¨‹å¸ˆ")

@cl.action_callback("select_leijun")
async def on_select_leijun(action: cl.Action):
    """åˆ‡æ¢ä¸ºé›·å†›æ¨¡å¼"""
    await switch_role("é›·å†›")

async def switch_role(role_name: str):
    """åˆ‡æ¢è§’è‰²"""
    cl.user_session.set("current_role", role_name)

    role_icon = {
        "äº§å“ç»ç†": "ğŸ“Š",
        "ç ”å‘å·¥ç¨‹å¸ˆ": "ğŸ’»",
        "é›·å†›": "ğŸš€"
    }.get(role_name, "ğŸ­")

    confirm_msg = cl.Message(
        content=f"{role_icon} å·²åˆ‡æ¢ä¸º **{role_name}** è§†è§’",
        author="ç³»ç»Ÿ"
    )
    await confirm_msg.send()

    await show_status()

# ============ æ¸…ç†å›è°ƒ ============
@cl.action_callback("clear_history")
async def on_clear_history(action: cl.Action):
    """æ¸…ç©ºå†å²è®°å½•"""
    cl.user_session.set("history", [])

    clear_msg = cl.Message(
        content="ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…ç©º",
        author="ç³»ç»Ÿ"
    )
    await clear_msg.send()

    await show_status()

# ============ ä¸»ç¨‹åº ============
if __name__ == "__main__":
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("ğŸ¤– AI å¯¹è¯åŠ©æ‰‹ - ç®€æ´ç‰ˆ")
    print("=" * 60)
    print(f"æ¨¡å‹: {CONFIG['model_name']}")
    print(f"è§’è‰²: {list(ROLE_PROMPTS.keys())}")
    print("=" * 60)
    print("å¯åŠ¨å‘½ä»¤: chainlit run app.py -w")
    print("è®¿é—®åœ°å€: http://localhost:8000")
    print("=" * 60)

    # æ³¨æ„ï¼šChainlit ä¼šè‡ªåŠ¨å¯åŠ¨æœåŠ¡å™¨
    # è¿è¡Œæ­¤æ–‡ä»¶æ—¶ï¼Œéœ€è¦åœ¨å‘½ä»¤è¡Œæ‰§è¡Œ: chainlit run app.py