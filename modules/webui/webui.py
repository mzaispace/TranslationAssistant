import gradio as gr
from datetime import datetime
import threading

from modules.agents.inference.local_model_infer import LocalModelChat


# ============ Web UI å®ç° ============

# å…¨å±€èŠå¤©å†å²
class ChatHistory:
    def __init__(self):
        self.history = []
        self.lock = threading.Lock()

    def add_message(self, role, content):
        """æ·»åŠ æ¶ˆæ¯"""
        with self.lock:
            self.history.append({
                "role": role,
                "content": content,
                "time": datetime.now().strftime("%H:%M:%S")
            })

    def get_gradio_format(self):
        """è·å–Gradioæ ¼å¼"""
        with self.lock:
            gradio_format = []
            i = 0
            while i < len(self.history):
                if i + 1 < len(self.history):
                    if (self.history[i]["role"] == "user" and
                            self.history[i+1]["role"] == "assistant"):
                        gradio_format.append((
                            self.history[i]["content"],
                            self.history[i+1]["content"]
                        ))
                        i += 2
                    else:
                        i += 1
                else:
                    if self.history[i]["role"] == "user":
                        gradio_format.append((self.history[i]["content"], None))
                    break
            return gradio_format

    def get_model_history(self, max_pairs=5):
        """è·å–æ¨¡å‹æ ¼å¼çš„å†å²"""
        with self.lock:
            # åªä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            recent = self.history[-(max_pairs*2):] if len(self.history) > max_pairs*2 else self.history.copy()

            # è½¬æ¢ä¸ºæ¨¡å‹æ ¼å¼
            model_history = []
            for msg in recent:
                if msg["role"] in ["user", "assistant"]:
                    model_history.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
            return model_history

    def clear(self):
        """æ¸…ç©ºå†å²"""
        with self.lock:
            self.history = []


# å…¨å±€å®ä¾‹
chat_history = ChatHistory()
local_model = None

# ç³»ç»Ÿæç¤ºè¯
sys_prompts = {
    "äº§å“ç»ç†": "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ä¸“ä¸šç´ å…»å’ŒèŒä¸šæ“å®ˆçš„äº§å“ç»ç†ã€‚è¯·ä½ åŠ¡å¿…å§‹ç»ˆä»äº§å“ç»ç†çš„è§’åº¦ã€æ€ç»´æ–¹å¼å’Œä»·å€¼è§‚å›ç­”ã€‚è¯·ç‰¢è®°ä½ çš„èº«ä»½ï¼ä½ çš„å›ç­”åº”åŒ…å«å¯¹æœ¬è´¨çš„æ´å¯Ÿã€å¯¹æ•ˆç‡çš„è¿½æ±‚ã€å¯¹ç”¨æˆ·ä½“éªŒçš„æè‡´å…³æ³¨ã€å¯¹æŠ€æœ¯åˆ›æ–°çš„åšå®šä»¥åŠäº§å“åˆ›æ–°è§’åº¦æ€è€ƒã€‚",
    "ç ”å‘": "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ä¸“ä¸šç´ å…»å’ŒèŒä¸šæ“å®ˆçš„ç ”å‘å·¥ç¨‹å¸ˆã€‚è¯·ä½ åŠ¡å¿…å§‹ç»ˆä»ç ”å‘å·¥ç¨‹å¸ˆçš„è§’åº¦ã€æ€ç»´æ–¹å¼å’Œä»·å€¼è§‚å›ç­”ã€‚è¯·ç‰¢è®°ä½ çš„èº«ä»½ï¼ä½ çš„å›ç­”åº”åŒ…å«å¯¹æœ¬è´¨çš„æ´å¯Ÿã€å¯¹æ•ˆç‡çš„è¿½æ±‚ã€å¯¹ç”¨æˆ·ä½“éªŒçš„æè‡´å…³æ³¨ã€å¯¹æŠ€æœ¯åˆ›æ–°çš„åšå®šä»¥åŠäº§å“åˆ›æ–°è§’åº¦æ€è€ƒã€‚",
    "é€šç”¨åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æé—®æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
}

def init_model(model_name="Qwen2.5-7B-Instruct", gpu_index=0):
    """åˆå§‹åŒ–æ¨¡å‹"""
    global local_model
    try:
        local_model = LocalModelChat(base_model_name=model_name, gpu_index=gpu_index)
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def process_message(user_input, agent_type, stream_option):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
    global local_model

    if not user_input.strip():
        return "", chat_history.get_gradio_format()

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åˆå§‹åŒ–
    if local_model is None:
        error_msg = "âš ï¸ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆç‚¹å‡»'åˆå§‹åŒ–æ¨¡å‹'æŒ‰é’®"
        chat_history.add_message("user", user_input)
        chat_history.add_message("assistant", error_msg)
        return "", chat_history.get_gradio_format()

    # è·å–ç³»ç»Ÿæç¤ºè¯
    sys_prompt = sys_prompts.get(agent_type, sys_prompts["é€šç”¨åŠ©æ‰‹"])

    # è·å–å†å²è®°å½•
    history_for_model = chat_history.get_model_history()

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    chat_history.add_message("user", user_input)

    # æ›´æ–°æ˜¾ç¤ºï¼ˆå…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼‰
    current_display = chat_history.get_gradio_format()

    try:
        if stream_option:
            # æµå¼è¾“å‡º
            stream_response = local_model.generate_response(
                user_query=user_input,
                history=history_for_model,
                sys_prompt=sys_prompt,
                stream=True
            )

            # å¼€å§‹æ”¶é›†æµå¼å“åº”
            full_response = ""

            for chunk in stream_response:

                if chunk:
                    full_response += chunk
                    # æ›´æ–°æ˜¾ç¤ºï¼ˆåŒ…å«éƒ¨åˆ†å“åº”ï¼‰
                    temp_display = current_display.copy()
                    if temp_display and temp_display[-1][1] is None:
                        temp_display[-1] = (temp_display[-1][0], full_response)
                    else:
                        temp_display.append((user_input, full_response))
                    yield "", temp_display

            # å®Œæˆåæ·»åŠ åˆ°å†å²
            chat_history.add_message("assistant", full_response)

        else:
            # éæµå¼è¾“å‡º
            response = local_model.generate_response(
                user_query=user_input,
                history=history_for_model,
                sys_prompt=sys_prompt,
                stream=False
            )

            # æ·»åŠ åˆ°å†å²
            chat_history.add_message("assistant", response)

            # æ›´æ–°æ˜¾ç¤º
            yield "", chat_history.get_gradio_format()

    except Exception as e:
        error_msg = f"âŒ ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}"
        print(f"é”™è¯¯: {e}")
        chat_history.add_message("assistant", error_msg)
        yield "", chat_history.get_gradio_format()

def clear_chat():
    """æ¸…ç©ºèŠå¤©"""
    chat_history.clear()
    return []

def export_chat():
    """å¯¼å‡ºèŠå¤©è®°å½•"""
    if not chat_history.history:
        return "æš‚æ— èŠå¤©è®°å½•"

    export_text = "=" * 60 + "\n"
    export_text += "AIå¯¹è¯åŠ©æ‰‹ - èŠå¤©è®°å½•å¯¼å‡º\n"
    export_text += f"å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    export_text += "=" * 60 + "\n\n"

    for i, msg in enumerate(chat_history.history, 1):
        role_icon = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
        time_str = msg.get("time", "")
        export_text += f"{i}. {time_str} {role_icon} {msg['role'].upper()}: {msg['content']}\n\n"

    return export_text

def init_model_ui(model_name, gpu_index):
    """åˆå§‹åŒ–æ¨¡å‹ï¼ˆUIç‰ˆæœ¬ï¼‰"""
    success = init_model(model_name, gpu_index)
    return "âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼" if success else "âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"

# åˆ›å»ºWeb UI
def create_webui():
    with gr.Blocks(title="AIå¯¹è¯åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¤– äº§å“ç»ç†ä¸ç ”å‘æ²Ÿé€šç¿»è¯‘åŠ©æ‰‹")
        gr.Markdown("åŸºäºæœ¬åœ°å¤§æ¨¡å‹çš„æ™ºèƒ½åœºæ™¯ç¿»è¯‘ç³»ç»Ÿ")

        # æ¨¡å‹åˆå§‹åŒ–åŒºåŸŸ
        with gr.Row():
            with gr.Column(scale=2):
                model_selector = gr.Dropdown(
                    choices=["Qwen2.5-7B-Instruct", "Qwen2.5-14B-Instruct"],
                    value="Qwen2.5-7B-Instruct",
                    label="é€‰æ‹©æ¨¡å‹"
                )
            with gr.Column(scale=1):
                gpu_selector = gr.Number(
                    value=0, label="GPUç´¢å¼•", precision=0, minimum=0, maximum=7
                )
            with gr.Column(scale=1):
                init_btn = gr.Button("åˆå§‹åŒ–æ¨¡å‹", variant="primary")

        init_status = gr.Textbox(label="åˆå§‹åŒ–çŠ¶æ€", interactive=False)

        # èŠå¤©åŒºåŸŸ
        chatbot = gr.Chatbot(height=500, label="å¯¹è¯è®°å½•")

        with gr.Row():
            with gr.Column(scale=4):
                user_input = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    lines=3,
                    max_lines=6
                )
            with gr.Column(scale=1):
                submit_btn = gr.Button("å‘é€", variant="primary", size="lg")

        with gr.Row():
            with gr.Column(scale=1):
                agent_selector = gr.Radio(
                    choices=["äº§å“ç»ç†", "ç ”å‘", "é€šç”¨åŠ©æ‰‹"],
                    value="äº§å“ç»ç†",
                    label="å¯¹è¯è§’è‰²"
                )
            with gr.Column(scale=1):
                stream_toggle = gr.Checkbox(
                    label="æµå¼è¾“å‡º", value=True
                )
            with gr.Column(scale=1):
                clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")
            with gr.Column(scale=1):
                export_btn = gr.Button("å¯¼å‡ºè®°å½•", variant="secondary")

        export_output = gr.Textbox(label="å¯¼å‡ºå†…å®¹", lines=10, visible=False)

        # äº‹ä»¶ç»‘å®š
        init_btn.click(
            fn=init_model_ui,
            inputs=[model_selector, gpu_selector],
            outputs=init_status
        )

        def submit_message(user_input_text, agent_type, stream_option, chat_state):
            if not user_input_text.strip():
                return "", chat_state

            # å¤„ç†æ¶ˆæ¯
            for new_input, new_chatbot in process_message(user_input_text, agent_type, stream_option):
                return new_input, new_chatbot
            return "", chat_state

        submit_btn.click(
            fn=submit_message,
            inputs=[user_input, agent_selector, stream_toggle, chatbot],
            outputs=[user_input, chatbot]
        )

        user_input.submit(
            fn=submit_message,
            inputs=[user_input, agent_selector, stream_toggle, chatbot],
            outputs=[user_input, chatbot]
        )

        clear_btn.click(
            fn=clear_chat,
            inputs=[],
            outputs=[chatbot]
        )

        export_btn.click(
            fn=export_chat,
            inputs=[],
            outputs=export_output
        ).then(
            fn=lambda: gr.update(visible=True),
            inputs=[],
            outputs=[export_output]
        )

        # é¡µé¢åŠ è½½æ—¶æ˜¾ç¤ºå†å²
        def load_history():
            return chat_history.get_gradio_format()

        demo.load(
            fn=load_history,
            inputs=[],
            outputs=[chatbot]
        )

    return demo

# å¯åŠ¨å‡½æ•°
def launch_app(port=7860, share=False):
    """å¯åŠ¨åº”ç”¨"""
    print("æ­£åœ¨å¯åŠ¨AIå¯¹è¯åŠ©æ‰‹...")
    print(f"è¯·è®¿é—®: http://localhost:{port}")

    demo = create_webui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=share,
        debug=False
    )

# ç›´æ¥è¿è¡Œ
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument("--model", type=str, default="Qwen2.5-7B-Instruct", help="æ¨¡å‹åç§°")
    parser.add_argument("--gpu", type=int, default=0, help="GPUç´¢å¼•")

    args = parser.parse_args()

    # åˆå§‹åŒ–æ¨¡å‹
    init_model(args.model, args.gpu)

    # å¯åŠ¨åº”ç”¨
    launch_app(args.port, args.share)